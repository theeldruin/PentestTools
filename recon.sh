#!/bin/bash

declare -i test=0
declare -i col=$(echo "$1" | sed 's/\./ /g' | awk '{print NF}')
declare alvo=$1
declare agente=$2
declare xssbind=$3

BLACK='\033[0;30m'
RED='\033[0;31m'
GREEN='\033[0;32m'
BO='\033[0;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
LGRAY='\033[0;37m'
DGRAY='\033[1;30m'
LRED='\033[1;31m'
LGREEN='\033[1;32m'
YELLOW='\033[1;33m'
LBLUE='\033[1;34m'
LPURPLE='\033[1;35m'
LCYAN='\033[1;36m'
WHITE='\033[1;37m'
NC='\033[0m'

if [[ -z agente || agente = 'TRUE' || agente = 'true' ]]
then
	agente="X-BugHunter: Myself"
fi

if [[ -z xssbind || xssbind = 'TRUE' || xssbind = 'true' ]]
then
	xssbind=""
fi

if [[ ($# -eq 2 && $2 = 'true') ]]
then
	$2 = 'TRUE'
fi

if [[ ($# -eq 3 && $3 = 'true') ]]
then
	$2 = 'TRUE'
fi
	
if [[ ($# -eq 4 && $4 = 'true') ]]
then
	$2 = 'TRUE'
fi
	
#Tools utilizadas = whatweb,gospider,subfinder,sublist3r,curl,eyewitness
#https://github.com/OWASP/Amass
#https://github.com/hahwul/dalfox
#https://github.com/KathanP19/Gxss
#https://github.com/tomnomnom/waybackurls
#https://github.com/s0md3v/uro
#https://github.com/lc/gau
#https://github.com/tomnomnom/assetfinder


ASN() {
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	### Enumerando ASN
	echo -e "${YELLOW}---> Enumerando ASN"
	asn -m -n $alvo | sed 's/\x1B\[[0-9;]\{1,\}[A-Za-z]//g' | sed 's/\x1b//g' | sed 's/(B//g' | sed 's/\[m//g' >> asn.txt
}

DNS() {
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	### Enumerando DNS
	echo -e "${RED}---> Enumerando DNS"
	#dnsrev.sh $alvo | sed 's/.*32m //' >> dns.txt
	dnsrev.sh $alvo >> dns.txt
}

WHOIS() {
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	#Enumerando whois
	echo -e "${GREEN}---> Enumerando Whois"
	whois $alvo | grep -v '%' > whois.txt
}

SUBDOMINIOS() {
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	### Enumerando Subdominios
	echo -e "${PURPLE}---> Enumerando Subdominios"
	echo -e "------> Running Subfinder"
	echo $alvo | subfinder -silent > /tmp/temp.txt

	echo -e "------> Running Sublist3r"
	touch /tmp/sub.txt
	sublist3r -d $alvo -o /tmp/sub.txt 2>/dev/null 1>/dev/null
	cat /tmp/sub.txt >> /tmp/temp.txt

	echo -e "------> Running Gau"
	gau $alvo --subs | sort | uro >> /tmp/gau.txt
	cat /tmp/gau.txt | sort | uro >> /tmp/links.txt
	cat /tmp/gau.txt | cut -d "/" -f 1,2,3 | sort | uro >> /tmp/temp.txt

	echo -e "------> Running OWASP Amass"
	amass enum -brute -silent -d $alvo >> /tmp/temp.txt
	
	echo -e "------> Running Assetfinder"
	echo $alvo | assetfinder --subs-only | uniq >> /tmp/temp.txt
		
	#xargs -a sitemap.txt -I@ sh -c 'curl $site -H "$agente" -p http://127.0.0.1:8080 -s >> /tmp/temp.txt'
	#xargs -a sitemap.txt -I@ sh -c 'curl $site -H "$agente" -s >> /tmp/temp.txt'

	### Filtrando subdominios encontrados e ativos
	echo -e "------> Verificando links que ainda estão ativos"
	cat /tmp/temp.txt | grep $alvo | sort | uniq | httpx -silent -retries 2 -sc -H "$agente" -fr -mc 200,403 | sed 's/\[.*$//' >> /tmp/subdomains.txt

	# Testando e removendo os com código de retorno HTTP 404
	#echo -e "------> Filtrando resultados pelo http_code, removendo status 404"
	#cat /tmp/subdomains.txt  | $HOME/go/bin/httpx -follow-host-redirects -random-agent -retries 2 -title -web-server -tech-detect -location -sc -H "$agente" -silent -fc 404 -proxy "http://127.0.0.1:8080" | sed 's/\[.*$//' | sort | uro > /tmp/temp.txt
	#cat /tmp/subdomains.txt  | $HOME/go/bin/httpx -follow-host-redirects -random-agent -retries 2 -title -web-server -tech-detect -location -sc -H "$agente" -silent -fc 404 | sed 's/\[.*$//' | sort | uro > /tmp/temp.txt

	# Registrando os subdominios ativos encontrados
	cat /tmp/subdomains.txt | grep $alvo | sort | uro > /tmp/tree.txt
	cat /tmp/tree.txt >> subdominios.txt
	
	#cat /tmp/links.txt | grep $alvo | sed 's/^.*http/http/g' | sed 's/\].*$//g' | sed 's/\/$//g' | sort | uro | grep $alvo | httpx -silent -retries 2 -sc -H "$agente" -fc 404 | grep -v 404 >> links.txt
	cat /tmp/links.txt | grep $alvo | sed 's/^.*http/http/g' | sed 's/\].*$//g' | sed 's/\/$//g' | sort | uro | grep $alvo | uniq >> links.txt
}

SUBDOMINIOSLEVEL2() {
	echo -e "${PURPLE}---> Enumerando Subdominios Level 2"
	for url in $(cat subdominios.txt | cut -d "/" -f 3 | uniq)
	do
		#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
		### Enumerando Subdominios
		echo -e "------> $url"
		echo -e "---------> Running Subfinder"
		echo $sub | subfinder -silent > /tmp/temp.txt

		echo -e "------> Running Sublist3r"
		touch /tmp/sub.txt
		sublist3r -d $sub -o /tmp/sub.txt 2>/dev/null 1>/dev/null
		cat /tmp/sub.txt >> /tmp/temp.txt

		echo -e "---------> Running Gau"
		gau $sub --subs | sort | uro >> /tmp/gau.txt
		cat /tmp/gau.txt | sort | uro >> /tmp/links.txt
		cat /tmp/gau.txt | cut -d "/" -f 1,2,3 | sort | uro >> /tmp/temp.txt

		echo -e "---------> Running OWASP Amass"
		amass enum -brute -silent -d $sub >> /tmp/temp.txt
		
		echo -e "---------> Running Assetfinder"
		echo $sub | assetfinder --subs-only | uniq >> /tmp/temp.txt
			
		#xargs -a sitemap.txt -I@ sh -c 'curl $site -H "$agente" -p http://127.0.0.1:8080 -s >> /tmp/temp.txt'
		#xargs -a sitemap.txt -I@ sh -c 'curl $site -H "$agente" -s >> /tmp/temp.txt'

		### Filtrando subdominios encontrados e ativos
		echo -e "---------> Verificando links que ainda estão ativos"
		cat /tmp/temp.txt | grep $sub | sort | uniq | httpx -silent -retries 2 -sc -H "$agente" -fr -mc 200,403 | sed 's/\[.*$//' >> /tmp/subdomains.txt

		# Testando e removendo os com código de retorno HTTP 404
		#echo -e "---------> Filtrando resultados pelo http_code, removendo status 404"
		#cat /tmp/subdomains.txt  | $HOME/go/bin/httpx -follow-host-redirects -random-agent -retries 2 -title -web-server -tech-detect -location -sc -H "$agente" -silent -fc 404 -proxy "http://127.0.0.1:8080" | sed 's/\[.*$//' | sort | uro > /tmp/temp.txt
		#cat /tmp/subdomains.txt  | $HOME/go/bin/httpx -follow-host-redirects -random-agent -retries 2 -title -web-server -tech-detect -location -sc -H "$agente" -silent -fc 404 | sed 's/\[.*$//' | sort | uro > /tmp/temp.txt

		# Registrando os subdominios ativos encontrados
		cat /tmp/subdomains.txt | grep $sub | sort | uro >> /tmp/tree.txt
		cat /tmp/tree.txt >> subdominios.txt
		
		#cat /tmp/links.txt | grep $sub | sed 's/^.*http/http/g' | sed 's/\].*$//g' | sed 's/\/$//g' | sort | uro | grep $sub | httpx -silent -retries 2 -sc -H "$agente" -fc 404 | grep -v 404 >> links.txt
		cat /tmp/links.txt | grep $sub | sed 's/^.*http/http/g' | sed 's/\].*$//g' | sed 's/\/$//g' | sort | uro | grep $sub | uniq >> links.txt
	done
	
	cat subdominios.txt > temp.txt
	cat temp.txt | sort | uro > subdominios.txt
	
	cat links.txt > temp.txt
	cat temp.txt | sort | uro > links.txt
	rm temp.txt
}

CRAWLING() {		
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	# Crawling
	echo -e "${CYAN}---> Crawling"
	echo -e "------> Running gospider"
	#gospider -S $(cat /tmp/tree.txt | sed 's/^.*\/\///g' | sort | uro) -d 10 -p http://127.0.0.1:8080 >> /tmp/spi.txt
	for teste in $(cat /tmp/tree.txt)
	do
		gospider -H "$agente" -s $teste -d 10 -c 1 -t 1 --other-source >> /tmp/spi.txt
	done
	
	cat /tmp/spi.txt | grep $alvo >> /tmp/spider.txt

	cat /tmp/spider.txt | grep "\[url\]" >> /tmp/url.txt
	cat /tmp/spider.txt | grep "\[linkfinder\]" >> /tmp/linkfinder.txt
	cat /tmp/spider.txt | grep "\[robots\]" >> /tmp/robots.txt
	cat /tmp/spider.txt | grep "\[form\]" >> /tmp/form.txt
	cat /tmp/spider.txt | grep "\[javascript\]" >> /tmp/javascript.txt

	# Organizando links encontrados encontradas
	#cat /tmp/linkfinder.txt | sed 's/.*from: http/http/'| egrep -v "MM|DD|YYYY|\.w3\.|- http|- application|- text|\.jpg|\.png" | cut -d ' ' -f4 | sed 's/\]//g' | sort | uro > /tmp/temp.txt
	#cat /tmp/robots.txt | cut -d " " -f3 | sort | uro >> /tmp/temp.txt
	#cat /tmp/url.txt | grep code-200 | cut -d " " -f5 | sort | uro >> /tmp/temp.txt

	#cat /tmp/linkfinder.txt | sed 's/^.*http/http/g' | sed 's/\].*$//g' | sort | uro > /tmp/temp.txt
	#cat /tmp/robots.txt | sed 's/^.*http/http/g' | sed 's/\].*$//g' | sort | uro >> /tmp/temp.txt
	#cat /tmp/url.txt | sed 's/^.*http/http/g' | sed 's/\].*$//g' | sort | uro >> /tmp/temp.txt
	#cat /tmp/links.txt | sed 's/^.*http/http/g' | sed 's/\].*$//g' | sort | uro >> /tmp/temp.txt

	cat /tmp/linkfinder.txt | sort | uro >> spider-linkfinder.txt
	cat /tmp/robots.txt | sort | uro >> spider-robots.txt
	cat /tmp/url.txt | sort | uro >> spider-url.txt
	cat /tmp/form.txt | sort | uro >> spider-form.txt
	cat /tmp/javascript.txt | sort | uro >> spider-javascript.txt

	cat /tmp/spider.txt >> /tmp/links.txt
	cat /tmp/tree.txt >> /tmp/links.txt
	cat /tmp/links.txt | grep $alvo | sed 's/^.*http/http/g' | sed 's/\].*$//g' | sed 's/\/$//g' | sort | uro | grep $alvo | httpx -silent -retries 2 -sc -H "$agente" -mc 200,403 >> links.txt
	cat links.txt | sed 's/\[.*$//g' | grep $alvo | sort | uro | grep $alvo | httpx -silent -retries 2 -H "$agente" -mc 200,403 -fr >> crawling.txt

	#Criando arquivos de subdominios
	cat crawling.txt | grep $alvo | cut -d "/" -f 1,2,3 | sed 's/ //' | sort | uro >> /tmp/subs.txt
	cat subdominios.txt >> /tmp/subs.txt
	cat /tmp/subs.txt | sort | uniq > subdominios.txt
}

HEADERS() {
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	#Enumerando headers
	echo -e "${YELLOW}---> Enumerando Headers"
	rm headers.txt 2>/dev/null

	for teste in $(cat subdominios.txt)
	do
		echo $teste >> headers.txt
		curl -H "$agente" -s -I $teste >> headers.txt
		echo >> headers.txt
	done
}

ARQUIVOS() {
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	#Coletando arquivos
	echo -e "${RED}---> Separando arquivos"
	cat links.txt | sed 's/\[.*$//g' |  grep "\.js[^a-zA-Z0-9_]" | sed 's/\.js.*$/\.js/g' | uro > js.txt
	cat links.txt | sed 's/\[.*$//g' |  grep "\.css[^a-zA-Z0-9_]" | sed 's/\.css.*$/\.css/g' | uro > css.txt
	cat links.txt | sed 's/\[.*$//g' |  grep "\.php[^a-zA-Z0-9_]" | sed 's/\.php.*$/\.php/g' | uro > php.txt
	cat links.txt | sed 's/\[.*$//g' |  grep "\.json[^a-zA-Z0-9_]" | sed 's/\.json.*$/\.json/g' | uro > json.txt
	cat links.txt | sed 's/\[.*$//g' |  grep "\.do[^a-zA-Z0-9_]" | sed 's/\.do.*$/\.do/g' | uro > do.txt
	cat links.txt | sed 's/\[.*$//g' |  grep "\.sql[^a-zA-Z0-9_]" | sed 's/\.sql.*$/\.sql/g' | uro > sql.txt
	cat links.txt | sed 's/\[.*$//g' |  grep "\.cgi[^a-zA-Z0-9_]" | sed 's/\.cgi.*$/\.cgi/g' | uro > cgi.txt
	
	cat links.txt | sed 's/\[.*$//g' |  grep "\.html[^a-zA-Z0-9_]" | sed 's/\.html.*$/\.html/g' | uro > html.txt
	cat links.txt | sed 's/\[.*$//g' |  grep "\.htm[^a-zA-Z0-9_]" | sed 's/\.htm.*$/\.htm/g' | uro >> html.txt
	cat links.txt | sed 's/\[.*$//g' |  grep "\.shtml[^a-zA-Z0-9_]" | sed 's/\.shtml.*$/\.shtml/g' | uro > shtml.txt
		
	cat links.txt | sed 's/\[.*$//g' |  grep "\.backup[^a-zA-Z0-9_]" | sed 's/\.backup.*$/\.backup/g' | uro > backup.txt
	cat links.txt | sed 's/\[.*$//g' |  grep "\.bkp[^a-zA-Z0-9_]" | sed 's/\.bkp.*$/\.bkp/g' | uro >> backup.txt
		
	cat links.txt | sed 's/\[.*$//g' | grep "=" | sed 's/^.*http/http/g' | cut -d " " -f1 | sed 's/\]//g' > params.txt
	cat links.txt | sed 's/\[.*$//g' | grep "wp-" | sed 's/^.*http/http/g' | cut -d " " -f1 | sed 's/\]//g' > wordpress.txt
	cat links.txt | sed 's/\[.*$//g' | grep "sitemap" | sed 's/^.*http/http/g' | cut -d " " -f1 | sed 's/\]//g' > sitemap.txt
	cat links.txt | sed 's/\[.*$//g' | grep "robots" | sed 's/^.*http/http/g' | cut -d " " -f1 | sed 's/\]//g' > robots.txt
}

WHATWEB() {
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	### Executando WhatWeb
	echo -e "${GREEN}---> WhatWeb"
	for teste in $(cat subdominios.txt)
	do
		whatweb $teste -H "$agente" -t 1 --log-brief=/tmp/whatweb.txt > /dev/null
		echo >> /tmp/whatweb.txt
	done

	cat /tmp/whatweb.txt | sed 's/\],/\]\n\t/g' | sed 's/\[200/\n\t\[200/g' | sed 's/\[302/\n\t\[302/g' | sed 's/Cookies/\n\tCookies/g' | sed 's/\t /\t/g' > whatweb.txt
}

NUCLEI() { #https://github.com/projectdiscovery/nuclei-templates
	echo -e "${PURPLE}---> Nuclei"
	#echo -e "------> Takeover" >> nuclei.txt
	#nuclei -p http://127.0.0.1:8080 -t takeovers -l subdominios.txt >> nuclei.txt
	nuclei -itags misc -l subdominios.txt -silent -nc >> nuclei.txt
}

TESTES() {
	echo -e "${CYAN}---> Paramspider"
	xargs -a subdominios.txt -I@ sh -c "paramspider -d @ --exclude js,jpg,eot,jpeg,gif,css,tif,tiff,png,ttf,woff,woff2,ico,pdf,svg,txt,font -l high | grep = >> /tmp/params.txt"

	cat /tmp/params.txt | egrep -v ".js|.jpg|.eot|.jpeg|.gif|.css|.tif|.tiff|.png|.ttf|.woff|.woff2|.ico|.pdf|.svg|.txt|.font" | Gxss -p BUGHUNTER -h "$agente" | sort | uro > params.txt

	echo -e "---> Dalfox (SQLi, SSTI, Open Redirect, CRLF Injection e XSS)"
	if [[ -z xssbind ]]
	then
		cat params.txt | dalfox pipe -S -H "$agente" -w 1 >> finds.txt
	else
		cat params.txt | dalfox pipe --blind $xssbind -S -H "$agente" -w 1 >> finds.txt
	fi

}

SCREENSHOTS() {
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	#Tirando screenshots
	echo -e "${YELLOW}---> Tirando Screenshots"
	{ eyewitness -f subdominios.txt --timeout 30 --no-prompt >> /dev/null; cp -s ./*/report.html .; }
}		

NOTIFY() {
	notify -silent -bulk -data finds.txt
	notify -silent -bulk -data nuclei.txt
}

recon () {
	rm -f /tmp/* 2>/dev/null
	rm -f $arquivo 2>/dev/null

	mkdir $alvo 2>/dev/null
	cd $alvo 2>/dev/null
	rm -rf *

	echo -e "${BO}---> Scan iniciado"
	echo -e "---> Enquanto aguarda a finalização visite os GitDorks do arquivo gdorks.txt e veja se aparece algo interessante ;)"
	gdork.sh $alvo >> gdork.txt
	echo		

	ASN
	DNS
	WHOIS
	SUBDOMINIOS
	SUBDOMINIOSLEVEL2
	#CRAWLING
	HEADERS
	ARQUIVOS
	WHATWEB
	
	if [[ ($# -eq 2 && $2 = 'TRUE') || ($# -eq 3 && $3 = 'TRUE') || ($# -eq 4 && $4 = 'TRUE') ]]
	then
		NUCLEI
		TESTES
		SCREENSHOTS
		NOTIFY
	fi
	
	echo -e "---> FIM DO RECONHECIMENTO, BOA SORTE \o/"
	
	rm $(find . -type f -empty -print)
	rm geckodriver.log 2>/dev/null
	rm /tmp/* 2>/dev/null
}

echo -e "${BLUE}   ___                                                    _       "
echo -e "  | _ \    ___     __      ___    _ _              ___   | |_     "
echo -e "  |   /   / -_)   / _|    / _ \  | ' \      _     (_-<   | ' \    "
echo -e "  |_|_\   \___|   \__|_   \___/  |_||_|   _(_)_   /__/_  |_||_|   "
echo -e "_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|  "
echo -e "\"'-0-0-'\"'-0-0-'\"'-0-0-'\"'-0-0-'\"'-0-0-'\"'-0-0-'\"'-0-0-'\"'-0-0-'  "
echo -e "                                                by Bruno \"Eldruin\""

if [[ $# -lt 1 || $col -lt 2 || $col -gt 4 || $# -gt 4 ]]
then
	echo -e "Este script irá fazer um reconhecimento do alvo selecionado, criar uma pasta com o nome do alvo e arquivos referentes a cada etapa do reconhecimento"
	echo -e "Por padrão as opções de verificação de vulnerabilidade vem desabilitadas, para utilizá-las coloque TRUE no último parametro que utilizar"
	echo -e "Forma de uso ---> ./recon.sh DOMINIO HEADER XSSBIND VULN?"
	echo -e "\t Exemplo 1: ./recon.sh google.com"
	echo -e "\t Exemplo 2: ./recon.sh google.com 'X-BugHunter: EuMesmo' 'https://eumesmo.xss.ht'"
	echo -e "\t Exemplo 3: ./recon.sh google.com 'X-BugHunter: EuMesmo' 'https://eumesmo.xss.ht' TRUE"
	echo -e "\t Exemplo 4: ./recon.sh google.com TRUE"
else
	declare -x host=$(echo $1 | cut -d "/" -f3 | cut -d " " -f1 )
	echo -e "${NC}"
	recon $alvo
fi		
