#!/bin/bash

declare -i test=0
declare -i col=$(echo "$1" | sed 's/\./ /g' | awk '{print NF}')
declare alvo=$1
declare agente=$2
declare xssbind=$3

if [[ -z agente ]]
then
	agente="X-BugHunter: Myself"
fi

#Tools utilizadas = whatweb,gospider,subfinder,sublist3r,curl,eyewitness
#https://github.com/incogbyte/shosubgo
#https://github.com/OWASP/Amass
#https://github.com/hahwul/dalfox
#https://github.com/KathanP19/Gxss
#https://github.com/devanshbatham/ParamSpider
#https://github.com/tomnomnom/waybackurls
#https://github.com/s0md3v/uro
#https://github.com/lc/gau

ASN() {
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	### Enumerando ASN
	echo "---> Enumerando ASN"
	asn -m -n $alvo | sed 's/\x1B\[[0-9;]\{1,\}[A-Za-z]//g' | sed 's/\x1b//g' | sed 's/(B//g' | sed 's/\[m//g' >> asn.txt
}

DNS() {
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	### Enumerando DNS
	echo "---> Enumerando DNS"
	#dnsrev.sh $alvo | sed 's/.*32m //' >> dns.txt
	dnsrev.sh $alvo >> dns.txt
}

SUBDOMINIOS() {
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	### Enumerando Subdominios
	echo "---> Enumerando Subdominios"
	echo "------> Running Subfinder"
	echo $alvo | subfinder -silent > /tmp/temp.txt

	echo "------> Running Sublist3r"
	touch /tmp/sub.txt
	sublist3r -d $alvo -o /tmp/sub.txt 2>/dev/null 1>/dev/null
	cat /tmp/sub.txt >> /tmp/temp.txt

	echo "------> Running Gau"
	gau $alvo --subs | sort | uro >> /tmp/gau.txt
	cat /tmp/gau.txt | sort | uro >> /tmp/links.txt
	cat /tmp/gau.txt | cut -d "/" -f 1,2,3 | sort | uro >> /tmp/temp.txt

	echo "------> Running OWASP Amass"
	amass enum -brute -silent -d $alvo >> /tmp/temp.txt
	
	echo "------> Running Aesstfinder"
	echo $alvo | assetfinder --subs-only | httpx -silent -fr -cl -sc -mc 200,403 -H "$agente" -t 1 -rl 1 >> /tmp/temp.txt
		
	#xargs -a sitemap.txt -I@ sh -c 'curl $site -H "$agente" -p http://127.0.0.1:8080 -s >> /tmp/temp.txt'
	#xargs -a sitemap.txt -I@ sh -c 'curl $site -H "$agente" -s >> /tmp/temp.txt'

	### Filtrando subdominios encontrados e ativos
	echo "------> Verificando links que ainda estão ativos"
	cat /tmp/temp.txt | grep $alvo | sort | uro > /tmp/temp2.txt
	cat /tmp/temp2.txt | uro | httpx -silent -retries 2 -sc -H "$agente" -fr -mc 200,403 | sed 's/\[.*$//' >> /tmp/subdomains.txt

	# Testando e removendo os com código de retorno HTTP 404
	#echo "------> Filtrando resultados pelo http_code, removendo status 404"
	#cat /tmp/subdomains.txt  | $HOME/go/bin/httpx -follow-host-redirects -random-agent -retries 2 -title -web-server -tech-detect -location -sc -H "$agente" -silent -fc 404 -proxy "http://127.0.0.1:8080" | sed 's/\[.*$//' | sort | uro > /tmp/temp.txt
	#cat /tmp/subdomains.txt  | $HOME/go/bin/httpx -follow-host-redirects -random-agent -retries 2 -title -web-server -tech-detect -location -sc -H "$agente" -silent -fc 404 | sed 's/\[.*$//' | sort | uro > /tmp/temp.txt

	# Registrando os subdominios ativos encontrados
	cat /tmp/subdomains.txt | grep $alvo | sort | uro > /tmp/tree.txt
	cat /tmp/subdomains.txt | grep $alvo | sort | uro >> subdominios.txt
	
	cat /tmp/links.txt | grep $alvo | sed 's/^.*http/http/g' | sed 's/\].*$//g' | sed 's/\/$//g' | sort | uro | grep $alvo | httpx -silent -retries 2 -sc -H "$agente" -fc 404 | grep -v 404 >> links.txt
}

CRAWLING() {		
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	# Crawling
	echo "---> Crawling"
	echo "------> Running gospider"
	#gospider -S $(cat /tmp/tree.txt | sed 's/^.*\/\///g' | sort | uro) -d 10 -p http://127.0.0.1:8080 >> /tmp/spi.txt
	for teste in $(cat /tmp/tree.txt)
	do
		gospider -H "$agente" -s $teste -d 10 -c 1 -t 1 --other-source >> /tmp/spi.txt
	done
	
	cat /tmp/spi.txt | grep $alvo >> /tmp/spider.txt

	cat /tmp/spider.txt | grep "\[url\]" >> /tmp/url.txt
	cat /tmp/spider.txt | grep "\[linkfinder\]" >> /tmp/linkfinder.txt
	cat /tmp/spider.txt | grep "\[robots\]" >> /tmp/robots.txt
	cat /tmp/spider.txt | grep "\[form\]" >> /tmp/form.txt
	cat /tmp/spider.txt | grep "\[javascript\]" >> /tmp/javascript.txt

	# Organizando links encontrados encontradas
	#cat /tmp/linkfinder.txt | sed 's/.*from: http/http/'| egrep -v "MM|DD|YYYY|\.w3\.|- http|- application|- text|\.jpg|\.png" | cut -d ' ' -f4 | sed 's/\]//g' | sort | uro > /tmp/temp.txt
	#cat /tmp/robots.txt | cut -d " " -f3 | sort | uro >> /tmp/temp.txt
	#cat /tmp/url.txt | grep code-200 | cut -d " " -f5 | sort | uro >> /tmp/temp.txt

	#cat /tmp/linkfinder.txt | sed 's/^.*http/http/g' | sed 's/\].*$//g' | sort | uro > /tmp/temp.txt
	#cat /tmp/robots.txt | sed 's/^.*http/http/g' | sed 's/\].*$//g' | sort | uro >> /tmp/temp.txt
	#cat /tmp/url.txt | sed 's/^.*http/http/g' | sed 's/\].*$//g' | sort | uro >> /tmp/temp.txt
	#cat /tmp/links.txt | sed 's/^.*http/http/g' | sed 's/\].*$//g' | sort | uro >> /tmp/temp.txt

	cat /tmp/linkfinder.txt | sort | uro >> linkfinder.txt
	cat /tmp/robots.txt | sort | uro >> robots.txt
	cat /tmp/url.txt | sort | uro >> url.txt
	cat /tmp/form.txt | sort | uro >> form.txt
	cat /tmp/javascript.txt | sort | uro >> javascript.txt

	cat /tmp/spider.txt >> /tmp/links.txt
	cat /tmp/tree.txt >> /tmp/links.txt
	cat /tmp/links.txt | grep $alvo | sed 's/^.*http/http/g' | sed 's/\].*$//g' | sed 's/\/$//g' | sort | uro | grep $alvo | httpx -silent -retries 2 -sc -H "$agente" -mc 200,403 >> links.txt
	cat links.txt | sed 's/\[.*$//g' | grep $alvo | sort | uro | grep $alvo | httpx -silent -retries 2 -H "$agente" -mc 200,403 -fr >> crawling.txt

	#Criando arquivos de subdominios
	cat crawling.txt | grep $alvo | cut -d "/" -f 1,2,3 | sed 's/ //' | sort | uro >> subdominios.txt
}

HEADERS() {
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	#Enumerando headers
	echo "---> Enumerando Headers"
	rm headers.txt 2>/dev/null

	for teste in $(cat subdominios.txt)
	do
		echo $teste >> headers.txt
		curl -H "$agente" -s -I $teste >> headers.txt
		echo >> headers.txt
	done
}

WHOIS() {
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	#Enumerando whois
	echo "---> Enumerando Whois"
	whois $alvo | grep -v '%' > whois.txt
}

SCREENSHOTS() {
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	#Tirando screenshots
	echo "---> Tirando Screenshots"
	{ eyewitness -f subdominios.txt --timeout 30 --no-prompt >> /dev/null; cp -s ./*/report.html .; }
}		

ARQUIVOS() {
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	#Coletando arquivos
	echo "---> Separando arquivos"
	cat links.txt | sed 's/\[.*$//g' |  grep ".js[^a-zA-Z0-9_]" | sed 's/\.js.*$/\.js/g' > js.txt
	cat links.txt | sed 's/\[.*$//g' |  grep ".css[^a-zA-Z0-9_]" | sed 's/\.css.*$/\.css/g' > js.txt
	cat links.txt | sed 's/\[.*$//g' |  grep ".php[^a-zA-Z0-9_]" | sed 's/\.php.*$/\.php/g' > js.txt
	cat links.txt | sed 's/\[.*$//g' |  grep ".json[^a-zA-Z0-9_]" | sed 's/\.json.*$/\.json/g' > js.txt
	cat links.txt | sed 's/\[.*$//g' |  grep ".do[^a-zA-Z0-9_]" | sed 's/\.do.*$/\.do/g' > js.txt
	cat links.txt | sed 's/\[.*$//g' |  grep ".html[^a-zA-Z0-9_]" | sed 's/\.html.*$/\.html/g' > js.txt
	
	cat links.txt | sed 's/\[.*$//g' | grep "=" | sed 's/^.*http/http/g' | cut -d " " -f1 | sed 's/\]//g' > params.txt
	cat links.txt | sed 's/\[.*$//g' | grep "wp-" | sed 's/^.*http/http/g' | cut -d " " -f1 | sed 's/\]//g' > wordpress.txt
	cat links.txt | sed 's/\[.*$//g' | grep "sitemap" | sed 's/^.*http/http/g' | cut -d " " -f1 | sed 's/\]//g' > sitemap.txt
	cat links.txt | sed 's/\[.*$//g' | grep "robots" | sed 's/^.*http/http/g' | cut -d " " -f1 | sed 's/\]//g' > robots.txt
}

WHATWEB() {
	#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	### Executando WhatWeb
	echo "---> WhatWeb"
	for teste in $(cat subdominios.txt)
	do
		whatweb $teste -H "$agente" -t 1 --log-brief=/tmp/whatweb.txt > /dev/null
		echo >> /tmp/whatweb.txt
	done

	cat /tmp/whatweb.txt | sed 's/\],/\]\n\t/g' | sed 's/\[200/\n\t\[200/g' | sed 's/\[302/\n\t\[302/g' | sed 's/Cookies/\n\tCookies/g' | sed 's/\t /\t/g' > whatweb.txt
}

NUCLEI() { #https://github.com/projectdiscovery/nuclei-templates
	echo "---> Nuclei"
	#echo "------> Takeover" >> nuclei.txt
	#nuclei -p http://127.0.0.1:8080 -t takeovers -l subdominios.txt >> nuclei.txt
	nuclei -itags misc -l subdominios.txt -silent -nc >> nuclei.txt
}

TESTES() {
	echo "---> Paramspider"
	xargs -a subdominios.txt -I@ sh -c "paramspider -d @ --exclude js,jpg,eot,jpeg,gif,css,tif,tiff,png,ttf,woff,woff2,ico,pdf,svg,txt,font -l high | grep = >> /tmp/params.txt"

	cat /tmp/params.txt | egrep -v ".js|.jpg|.eot|.jpeg|.gif|.css|.tif|.tiff|.png|.ttf|.woff|.woff2|.ico|.pdf|.svg|.txt|.font" | Gxss -p eldruin -h "$agente" | sort | uro > params.txt

	echo "---> Dalfox (SQLi,SSTI, Open Redirect, CRLF Injection e XSS)"
	if [[ -z xssbind ]]
	then
		cat params.txt | dalfox pipe -S -H "$agente" -w 1 >> finds.txt
	else
		cat params.txt | dalfox pipe --blind $xssbind -S -H "$agente" -w 1 >> finds.txt
	fi

}

NOTIFY() {
	notify -silent -bulk -data finds.txt
	notify -silent -bulk -data nuclei.txt
}

recon () {
	rm -f /tmp/* 2>/dev/null
	rm -f $arquivo 2>/dev/null

	mkdir $alvo 2>/dev/null
	cd $alvo 2>/dev/null
	rm -rf *

	echo "---> Scan iniciado"
	echo "---> Enquanto aguarda a finalização visite os GitDorks do arquivo gdorks.txt e veja se aparece algo interessante ;)"
	gdork.sh $alvo >> gdork.txt
	echo		

	ASN
	DNS
	WHOIS
	SUBDOMINIOS
	#CRAWLING
	HEADERS
	#SCREENSHOTS
	ARQUIVOS
	WHATWEB
	#NUCLEI
	#TESTES
	#NOTIFY

	rm geckodriver.log 2>/dev/null
	rm /tmp/* 2>/dev/null
}

echo "   ___                                                    _       "
echo "  | _ \    ___     __      ___    _ _              ___   | |_     "
echo "  |   /   / -_)   / _|    / _ \  | ' \      _     (_-<   | ' \    "
echo "  |_|_\   \___|   \__|_   \___/  |_||_|   _(_)_   /__/_  |_||_|   "
echo "_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|  "
echo "\"`-0-0-'\"`-0-0-'\"`-0-0-'\"`-0-0-'\"`-0-0-'\"`-0-0-'\"`-0-0-'\"`-0-0-'  "
echo "                                                by Bruno \"Eldruin\""
echo "                                                                                                                                                                                               "
if [[ $# -lt 1 || $col -lt 2 || $col -gt 4 || $# -gt 3 ]]
then
	echo -e "Bem vindo ao Recon.sh\n"
	echo -e "Este script irá fazer um reconhecimento do alvo selecionado, criar uma pasta com o nome do alvo e arquivos referentes a cada etapa do reconhecimento"
	echo -e "Forma de uso ---> ./recon.sh DOMINIO HEADER XSSBIND"
	echo -e "\t Exemplo: ./recon.sh google.com"
	echo -e "\t Exemplo: ./recon.sh google.com 'X-BugHunter: EuMesmo' 'https://eumesmo.xss.ht'"
	echo
else
	declare -x host=$(echo $1 | cut -d "/" -f3 | cut -d " " -f1 )
	recon $alvo
fi		
